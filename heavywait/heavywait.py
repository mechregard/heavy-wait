import os
import asyncio
from pathlib import Path
from datetime import datetime
import openai
from typing import Any, Dict, List, Tuple, Optional
import logging
from langchain import LLMChain
from langchain.schema import LLMResult
from langchain import PromptTemplate
from langchain.llms import OpenAI
from langchain.callbacks import get_openai_callback
from heavywait.prompts import Prompts


class HeavyWait:
    """
    Heavy-wait is the lumbering companion to light-wait. Light-wait generates a static blog site
    from markdown, including label (tag) based navigation.
    The metadata used by light-wait, the labels, title, etc, is automatically generated by
    heavy-wait via LLM magic. Additionally, summary, keywords and links are generated.

    Makes use of OpenAI API
    TODO support on-prem model
    """
    BASE_MODEL = "text-davinci-003"
    CHAT_MODEL = "gpt-3.5-turbo"

    def __init__(self, llm: Optional[object] = None):
        """
        External llm can be a mock for testing
        :param llm:
        """
        openai.organization = os.getenv('OPENAI_ORG', 'gbgsd')
        openai.api_key = os.getenv("OPENAI_API_KEY")
        logging.basicConfig(format='%(levelname)s %(asctime)s %(module)s: %(message)s',
                            datefmt='%Y-%m-%d,%H:%M:%S',
                            level=logging.WARN)
        self.llm_map = {
            Prompts.MOOD_STRICT:
                OpenAI(model_name=HeavyWait.BASE_MODEL, temperature=0.1) if llm is None else llm,
            Prompts.MOOD_EXPRESSIVE:
                OpenAI(model_name=HeavyWait.BASE_MODEL, temperature=0.8) if llm is None else llm
        }

    def directory(self, src: Path, dst: Path):
        """
        Given a path to a directory of markdown files, process each

        :param src:
        :param dst:
        :return:
        """
        for src_file in src.glob('**/*.md'):
            dst_file = Path(dst, src_file.name)
            if not self._is_decorated(src_file):
                self.file(src_file, dst_file)

    def file(self, src: Path, dst: Path):
        """
        Given a path to a markdown file, generate new file with 'md' extension

        :param src:
        :param dst:
        :return:
        """
        logging.info(f"Decorate file {src} {dst}")
        date = HeavyWait._discover_date(src)
        with open(src) as f:
            content = f.read()
        response = self.decoration(content)
        markdown = "description: " + response['title'].strip() + '\n'
        markdown += "tags: " + response['labels'].strip() + '\n'
        markdown += "date: " + date + '\n'
        markdown += "keywords: " + response['keywords'].strip() + '\n\n'
        markdown += response['summary'].strip() + '\n\n'
        markdown += content + '\n'
        markdown += response['links'] + '\n'
        with open(dst, 'w') as ff:
            ff.write(markdown)
        print(f"Wrote {dst}")

    def decoration(self, markdown: str) -> Dict[str, str]:
        """
        Given markdown text, answer back a dictionary containing decorations for the markdown:
         * title: short description
         * keywords: top 5 in doc
         * summary: detailed summary- used as context for labels and links
         * labels: pick most relevant from given set of tags based on summary
         * supporting links: based on summary

        :param markdown: content
        :return:
        """
        return self._async_chains(markdown)

    @staticmethod
    def _is_decorated(src_file: Path) -> bool:
        """
        description:So someone mentioned razor
        tags:product
        date:15 May 2020
        :return:
        """
        with open(src_file) as f:
            if f.readline().startswith("description:"):
                return True
        return False

    @staticmethod
    def _discover_date(file_path: Path) -> str:
        stamp = datetime.fromtimestamp(Path(file_path).stat().st_mtime)
        return stamp.strftime("%d %b %Y")

    @staticmethod
    async def _async_generate(chain: LLMChain, input_list: List[Dict[str, str]], results: Dict[str, Any]):
        """
        Given a chain and input list of dictionaries and results dictionary, create an async callback
        and capture the results.
        :param chain:
        :param input_list:
        :param results:
        :return:
        """
        logging.info(f"Async chain generate {chain.input_keys=} {chain.output_keys=}")
        # async generate returns LLMResult which is a collection of tuples
        llm_result: LLMResult = await chain.agenerate(input_list)
        results[chain.output_key] = llm_result.generations[0][0].text

    async def _task_group(self, chain_params: Dict[str, Tuple], chain_input: List[Dict[str, str]]):
        """
        Create a group of tasks (like python 11) where each task is a chain created from given
        chain_params. These params map an output key to the tuple (MOOD, prompt) used to generate it.
        The MOOD allows us to have prompts with different 'temperature' requirements.
        :param chain_params:
        :param chain_input:
        :return:
        """
        group_results = {}
        tg = []
        for k,tup in chain_params.items():
            chain = LLMChain(llm=self.llm_map[tup[0]],
                             prompt=PromptTemplate.from_template(tup[1]),
                             output_key=k)
            tg.append(HeavyWait._async_generate(chain, chain_input, group_results))
        await asyncio.gather(*tg)
        return group_results

    def _async_chains(self, text: str):
        """
        Create groups of async chains. The first group all use the source markdown as part of the input.
        The second group uses the smaller summary output from the first group. This not only reduces
        the total amount of tokens sent: it helps align the output of the second group.
        :param text:
        :return:
        """
        with get_openai_callback() as cb:
            overall = {}
            summary_group = {
                "title": Prompts.get_title_prompt(),
                "keywords": Prompts.get_keywords_prompt(),
                "summary": Prompts.get_concise_summary_prompt()
            }
            summary_input = [{"text": text}]
            summary_results = asyncio.run(self._task_group(summary_group, summary_input))
            overall.update(summary_results)
            labels_group = {
                "labels": Prompts.get_labels_prompt(),
                "links": Prompts.get_links_prompt()
            }
            labels_input = [{"summary": summary_results['summary']}]
            labels_results = asyncio.run(self._task_group(labels_group, labels_input))
            overall.update(labels_results)

            logging.info(f"Total Tokens: {cb.total_tokens}")
            logging.info(f"Prompt Tokens: {cb.prompt_tokens}")
            logging.info(f"Completion Tokens: {cb.completion_tokens}")
            logging.info(f"Total Cost (USD): ${cb.total_cost}")

        return overall
